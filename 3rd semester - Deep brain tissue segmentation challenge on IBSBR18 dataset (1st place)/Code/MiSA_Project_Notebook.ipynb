{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"name":"MAIA seminar (Part 2).ipynb","colab":{"name":"MiSA_Project_Notebook.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"fqQJHMCWc-pY","colab_type":"text"},"source":["#Project - Tissue Segmentation on IBSR 18 dataset\n","Medical Image Segmentation and Application\n","\n","**Pierpaolo Vendittelli, Abdullah Thabit, Prem Prasad**\n","\n","*Universitat de Girona, Spain*\n","\n","For this Notebook to run, First add the shared folder **MISA_FinalProject_Abdullah_Pierpaolo_Prem** to your Google Drive by right clicking on the folder and choosing \"Add to my drive\"\n","\n","![](https://scontent-mxp1-1.xx.fbcdn.net/v/t1.0-9/12096563_522871731212414_7578645762495100106_n.jpg?_nc_cat=106&_nc_ohc=TM1Ii9lGFqUAQmpaOngYsEmYv_Y7SJa1XWpJs2i_5MSisAkDtE71cCg3g&_nc_ht=scontent-mxp1-1.xx&oh=e5271d359fe5f51b2a70983a38d2cedc&oe=5E6A391C)\n","\n"]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"8V1N2n1gc-pg","colab_type":"text"},"source":["# Dependencies and Architectures:\n","\n","---\n","**Connecting with Drive, checking Directories, importing dependencies**\n","\n"]},{"cell_type":"code","metadata":{"id":"W_g111qxc93h","colab_type":"code","outputId":"30d4f89f-6043-47af-9c37-420b6ccdb1da","executionInfo":{"status":"ok","timestamp":1578715476244,"user_tz":-60,"elapsed":51726,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":481}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","%cd 'drive/My Drive'\n","%cd 'MISA_FinalProject_Abdullah_Pierpaolo_Prem'\n","\n","print('Importing Dependencies')\n","!pip install statsmodels\n","!pip install barbar\n","!pip install https://github.com/ANTsX/ANTsPy/releases/download/v0.1.8/antspy-0.1.7-cp36-cp36m-linux_x86_64.whl\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive\n","/content/drive/My Drive/MISA_FinalProject_Abdullah_Pierpaolo_Prem\n","Importing Dependencies\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.10.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.17.5)\n","Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.25.3)\n","Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.1)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.4.1)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels) (2.6.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels) (1.12.0)\n","Collecting barbar\n","  Downloading https://files.pythonhosted.org/packages/48/1f/9b69ce144f484cfa00feb09fa752139658961de6303ea592487738d0b53c/barbar-0.2.1-py3-none-any.whl\n","Installing collected packages: barbar\n","Successfully installed barbar-0.2.1\n","Collecting antspy==0.1.7\n","\u001b[?25l  Downloading https://github.com/ANTsX/ANTsPy/releases/download/v0.1.8/antspy-0.1.7-cp36-cp36m-linux_x86_64.whl (255.7MB)\n","\u001b[K     |████████████████████████████████| 255.7MB 65kB/s \n","\u001b[?25hInstalling collected packages: antspy\n","Successfully installed antspy-0.1.7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Bum4UaHrLgiM","colab_type":"code","outputId":"17b19e00-0987-4822-8f28-a519bb2a8ad9","executionInfo":{"status":"ok","timestamp":1578715489997,"user_tz":-60,"elapsed":36955,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["import os\n","import sys\n","import random\n","import ants\n","import logging\n","import torch\n","import numpy as np\n","import json\n","from utils import MRI_DataPatchLoader\n","from torch.utils.data import DataLoader\n","from torch.optim import Adam\n","from utils import get_inference_patches, reconstruct_image\n","import pandas as pd\n","from metrics import DSC_seg, TPF_det, PPV_det\n","import nibabel as nib\n","import matplotlib.pyplot as plt\n","import copy\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import time\n","from barbar import Bar\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n","  \"(https://pypi.org/project/six/).\", FutureWarning)\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"yUTn691KMG23","colab_type":"text"},"source":["Create a folder to save the experiment's files"]},{"cell_type":"code","metadata":{"id":"TUjTUyctMMMK","colab_type":"code","colab":{}},"source":["# -----------------------------\n","# create a folder for the experiment\n","# -----------------------------\n","tmpdir = \"experiments\"\n","if not (os.path.exists(tmpdir)):\n","    os.mkdir(tmpdir)\n","\n","name = \"experiment_checkCode\"\n","tmpdir = os.path.join(tmpdir, name)\n","if not (os.path.exists(tmpdir)):\n","    os.mkdir(tmpdir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wunC21DCZCVB","colab_type":"text"},"source":["## ResUnet\n","\n","This Model is adapted from the work of Glioma Segmentation in https://github.com/karinvangarderen/glassimaging\n","\n","<img src=\"https://www.researchgate.net/publication/335599509/figure/fig2/AS:799250594152448@1567567599628/Res-UNet-architecture-On-the-picture-left-side-the-residual-block-configurations.ppm\" width=\"900\" height=\"600\" />\n","\n"]},{"cell_type":"code","metadata":{"id":"InwGM3KEZFhL","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","3D with ResNet blocks\n","\"\"\"\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class ResUNetBody(nn.Module):\n","\n","    def __init__(self, k=16, outputsize=2, inputsize=4):\n","        super(ResUNetBody, self).__init__()\n","        self.outputsize = outputsize\n","        self.inputsize = inputsize\n","        self. k = k\n","\n","        self.conv = ConvBlock(inputsize, k, 3)\n","        self.denseBlock1 = DenseBlock(n=2, inputsize=k)\n","        self.TD1 = TD(inputsize=k, outputsize=k*2)\n","        self.denseBlock2 = DenseBlock(n=2, inputsize=k*2)\n","        self.TD2 = TD(inputsize=k*2, outputsize=k*4)\n","        self.denseBlock3 = DenseBlock(n=2, inputsize=k*4)\n","        self.TD3 = TD(inputsize=k*4, outputsize=k * 4)\n","        self.denseBlock4 = DenseBlock(n=2, inputsize=k*4)\n","        self.TD4 = TD(inputsize=k*4, outputsize=k*4)\n","        self.denseBlockmid = DenseBlock(n=2, inputsize=k*4)\n","        self.UP1 = nn.ConvTranspose3d(k*4, k*4, 2, stride=2)\n","        self.denseBlock4_right = DenseBlock(n=2, inputsize=k*8)\n","        self.UP2 = nn.ConvTranspose3d(k*8, k*4, 2, stride=2)\n","        self.denseBlock3_right = DenseBlock(n=2, inputsize=k*8)\n","        self.UP3 = nn.ConvTranspose3d(k*8, k*2, 2, stride=2)\n","        self.denseBlock2_right = DenseBlock(n=2, inputsize=k*4)\n","        self.UP4 = nn.ConvTranspose3d(k*4, k*1, 2, stride=2)\n","        self.denseBlock1_right = DenseBlock(n=2, inputsize=k*2)\n","\n","    def forward(self, x):\n","        res = self.conv(x)\n","        res = self.denseBlock1(res)\n","        skip1 = res.clone()\n","        res = self.TD1(res)\n","        res = self.denseBlock2(res)\n","        skip2 = res.clone()\n","        res = self.TD2(res)\n","        res = self.denseBlock3(res)\n","        skip3 = res.clone()\n","        res = self.TD3(res)\n","        res = self.denseBlock4(res)\n","        skip4 = res.clone()\n","        res = self.TD4(res)\n","        res = self.denseBlockmid(res)\n","        res = self.UP1(res)\n","        skip4 = skip4\n","        res = torch.cat([res, skip4], dim=1)\n","        res = self.denseBlock4_right(res)\n","        res = self.UP2(res)\n","        skip3 = skip3\n","        res = torch.cat([res, skip3], dim=1)\n","        res = self.denseBlock3_right(res)\n","        res = self.UP3(res)\n","        skip2 = skip2\n","        res = torch.cat([res, skip2], dim=1)\n","        res = self.denseBlock2_right(res)\n","        res = self.UP4(res)\n","        skip1 = skip1\n","        res = torch.cat([res, skip1], dim=1)\n","        res = self.denseBlock1_right(res)\n","        return res\n","\n","\n","\n","\n","class ResUNet(nn.Module):\n","\n","    def __init__(self, k=16, outputsize=2, inputsize=4):\n","        super(ResUNet, self).__init__()\n","        self.outputsize = outputsize\n","        self.inputsize = inputsize\n","        self. k = k\n","        self.body = ResUNetBody(k=k, outputsize=outputsize, inputsize=inputsize)\n","        self.FC = ConvBlock(k*2, k*2, 1, padding=False)\n","        self.classifier = nn.Conv3d(k*2, self.outputsize, 1, padding=0)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, x):\n","        res = self.body(x)\n","        res = self.FC(res)\n","        res = self.classifier(res)\n","        res = self.softmax(res)\n","        return res\n","\n","\n","class DenseBlock(nn.Module):\n","\n","    def __init__(self, k=10, n=4, inputsize=32, normgroups=1):\n","        super(DenseBlock, self).__init__()\n","        self.k = k\n","        self.n = n\n","        self.inputsize = inputsize\n","        self.convolutions = nn.ModuleList([nn.Conv3d(inputsize, inputsize, 3, padding=1) for _ in range(0, self.n)])\n","        self.groupNorm = nn.ModuleList([nn.GroupNorm(inputsize, inputsize) for _ in range(0, self.n)])\n","        self.dropout_dense = nn.Dropout3d(p=0.1)\n","\n","    def forward(self, x):\n","        res = x\n","        for i in range(0, self.n):\n","            res = self.convolutions[i](res)\n","            res = self.groupNorm[i](res)\n","            res = F.leaky_relu(res)\n","            res = self.dropout_dense(res)\n","        res.add(x)\n","        return res\n","\n","    def getOutputImageSize(self, inputsize):\n","        outputsize = [i - (self.n * 2) for i in inputsize]\n","        return outputsize\n","\n","\n","class ConvBlock(nn.Module):\n","\n","    def __init__(self, channels_in, channels_out, kernel_size, dropout=False, batchnorm=True, instancenorm=True,\n","                 padding=True):\n","        super(ConvBlock, self).__init__()\n","        self.batchnorm = batchnorm\n","        self.dropout = dropout\n","        self.instancenorm = instancenorm\n","        if batchnorm:\n","            self.batchnorm_layer = nn.BatchNorm3d(channels_out)\n","        if padding:\n","            padding = 1\n","        else:\n","            padding = 0\n","        self.conv = nn.Conv3d(channels_in, channels_out, kernel_size, padding=padding)\n","        if dropout:\n","            self.dropout_layer = nn.Dropout3d(p=0.1)\n","        if instancenorm:\n","            self.instance_layer = nn.InstanceNorm3d(channels_in)\n","\n","    def forward(self, x):\n","        if self.dropout:\n","            x = self.dropout_layer(x)\n","        x = self.conv(x)\n","        if self.batchnorm:\n","            x = self.batchnorm_layer(x)\n","        if self.instancenorm:\n","            x = self.instance_layer(x)\n","        x = F.leaky_relu(x)\n","        return x\n","\n","\n","class TD(nn.Module):\n","\n","    def __init__(self, inputsize=32, outputsize=32):\n","        super(TD, self).__init__()\n","        self.inputsize = inputsize\n","        self.outputsize = outputsize\n","        self.convolution = nn.Conv3d(self.inputsize, self.outputsize, 3, stride=2, padding=1)\n","\n","    def forward(self, x):\n","        res = self.convolution(x)\n","        return res\n","\n","    def getOutputImageSize(self, inputsize):\n","        outputsize = [i // 2 for i in inputsize]\n","        return outputsize\n","\n","    def getOutputChannelSize(self):\n","        return self.k\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"fDV_Dmf3c-pp","colab_type":"text"},"source":["##  U-NET: \n","---\n","\n","This Model is adapted from the Unet Seminar https://github.com/sergivalverde/MAIA_seminar\n","\n","<img src=\"https://miro.medium.com/max/2640/1*J3t2b65ufsl1x6caf6GiBA.png\" width=\"600\" height=\"600\" />\n"," "]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"n10yXNLYc-pr","colab_type":"text"},"source":["Let's now define our first patch-wise `Unet` model in PyTorch:"]},{"cell_type":"code","metadata":{"autoscroll":false,"ein.hycell":false,"ein.tags":"worksheet-0","id":"IqV9KAupc-ps","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Unet(nn.Module):\n","    \"\"\"\n","    Basic U-net model\n","    \"\"\"\n","\n","    def __init__(self, input_size, output_size, dropout_rate=0.2):\n","\n","        super(Unet, self).__init__()\n","\n","        # conv1 down\n","        self.conv1 = nn.Conv3d(in_channels=input_size,\n","                               out_channels=32,\n","                               kernel_size=3,\n","                               padding=1)\n","        \n","        # batchNorm_dropout 1\n","        self.batchnorm1 = nn.InstanceNorm3d(32)\n","        self.dropout1 = nn.Dropout3d(p=dropout_rate)\n","\n","        # max-pool 1\n","        self.pool1 = nn.Conv3d(in_channels=32,\n","                               out_channels=32,\n","                               kernel_size=2,\n","                               stride=2)\n","        # conv2 down\n","        self.conv2 = nn.Conv3d(in_channels=32,\n","                               out_channels=64,\n","                               kernel_size=3,\n","                               padding=1)\n","        \n","        # batchNorm_dropout 2\n","        self.batchnorm2 = nn.InstanceNorm3d(64)\n","        self.dropout2 = nn.Dropout3d(p=dropout_rate)\n","\n","        # max-pool 2\n","        self.pool2 = nn.Conv3d(in_channels=64,\n","                               out_channels=64,\n","                               kernel_size=2,\n","                               stride=2)\n","        # conv3 down\n","        self.conv3 = nn.Conv3d(in_channels=64,\n","                               out_channels=128,\n","                               kernel_size=3,\n","                               padding=1)\n","        \n","        # batchNorm_dropout 3\n","        self.batchnorm3 = nn.InstanceNorm3d(128)\n","        self.dropout3 = nn.Dropout3d(p=dropout_rate)\n","\n","        # max-pool 3\n","        self.pool3 = nn.Conv3d(in_channels=128,\n","                               out_channels=128,\n","                               kernel_size=2,\n","                               stride=2)\n","        # conv4 down (latent space)\n","        self.conv4 = nn.Conv3d(in_channels=128,\n","                               out_channels=256,\n","                               kernel_size=3,\n","                               padding=1)\n","        \n","        # batchNorm_dropout 4\n","        self.batchnorm4 = nn.InstanceNorm3d(256)\n","        self.dropout4 = nn.Dropout3d(p=dropout_rate)\n","\n","        # up-sample conv4\n","        self.up1 = nn.ConvTranspose3d(in_channels=256,\n","                                      out_channels=128,\n","                                      kernel_size=2,\n","                                      stride=2)        \n","        # conv 5 (add up1 + conv3)\n","        self.conv5 = nn.Conv3d(in_channels=128,\n","                               out_channels=128,\n","                               kernel_size=3,\n","                               padding=1)\n","        # up-sample conv5\n","        self.up2 = nn.ConvTranspose3d(in_channels=128,\n","                                      out_channels=64,\n","                                      kernel_size=2,\n","                                      stride=2)\n","        # conv6 (add up2 + conv2) \n","        self.conv6 = nn.Conv3d(in_channels=64,\n","                               out_channels=64,\n","                               kernel_size=3,\n","                               padding=1)\n","        # up 3\n","        self.up3 = nn.ConvTranspose3d(in_channels=64,\n","                                      out_channels=32,\n","                                      kernel_size=2,\n","                                      stride=2)\n","        # conv7 (add up3 + conv1)\n","        self.conv7 = nn.Conv3d(in_channels=32,\n","                               out_channels=32,\n","                               kernel_size=3,\n","                               padding=1)\n","        # conv8 (classification)\n","        self.conv8 = nn.Conv3d(in_channels=32,\n","                               out_channels=output_size,\n","                               kernel_size=1)\n","\n","    def forward(self, x):\n","\n","        # encoder\n","        x1 = F.relu(self.conv1(x))\n","        x1 = self.batchnorm1(x1)\n","        x1 = self.dropout1(x1)\n","        x1p = self.pool1(x1)\n","\n","        x2 = F.relu(self.conv2(x1p))\n","        x2 = self.batchnorm2(x2)\n","        x2 = self.dropout2(x2)        \n","        x2p = self.pool2(x2)\n","\n","        x3 = F.relu(self.conv3(x2p))\n","        x3 = self.batchnorm3(x3)\n","        x3 = self.dropout3(x3)\n","        x3p = self.pool3(x3)\n","        \n","        # latent space\n","        x4 = F.relu(self.conv4(x3p))\n","        x4 = self.batchnorm4(x4)\n","        x4 = self.dropout4(x4)\n","\n","        # decoder\n","        up1 = self.up1(x4)\n","        x5 = F.relu(self.conv5(up1 + x3)) # look how layers are added :o\n","        up2 = self.up2(x5)\n","        x6 = F.relu(self.conv6(up2 + x2))\n","        up3 = self.up3(x6)\n","        x7 = F.relu(self.conv7(up3 + x1))\n","        \n","        # output layer (2 classes)\n","        # we use a softmax layer to return probabilities for each class\n","        out = F.softmax(self.conv8(x7), dim=1) \n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"HfEP96Mwc-pw","colab_type":"text"},"source":["# 2. Data preparation:\n","---\n","\n","Let's now obtain some data to train and test the model. Download the MRI scans and move it to `data`:"]},{"cell_type":"code","metadata":{"autoscroll":false,"ein.hycell":false,"ein.tags":"worksheet-0","id":"xd2n2ZCOc-p1","colab_type":"code","outputId":"ccfb6ce0-2469-4fe7-eeaa-fd19133462a8","executionInfo":{"status":"ok","timestamp":1578715490473,"user_tz":-60,"elapsed":8957,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["basedir = os.getcwd()\n","print('My current directory is: ', basedir)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["My current directory is:  /content/drive/My Drive/MISA_FinalProject_Abdullah_Pierpaolo_Prem\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hCkGVr5Yc-p-","colab_type":"text"},"source":["\n","Let's now train our `Unet` model using the `WMH2017` data. Before start training, several steps have to be taken into account: \n","1. Split the available data into training, validation and test sets. \n","2. Build the proper training data structures using the PyTorch `Dataset` and `Dataloader` objects.\n","3. Build the training loop. \n","\n","With the aim  to unify all the hyper-parameters and training options, we will define a dictionary `options` that we will populate with new values: "]},{"cell_type":"markdown","metadata":{"id":"cs9nL8-0c-qB","colab_type":"text"},"source":["## 2.2 Training generators: \n","\n","We are going to use the helper class `MRI_DataPatchLoader` from `utils.py` to generate a `Dataset` object based on our data. `MRI_DataPathLoader` requires an input dictionary with all image paths, image labels, and some other options for patch generation and normalization that we will cover in a few minutes. \n","\n","Additionally, the `MRI_DataPathLoader` also requires a third dictionary containing the paths a ROI mask from within the patch sampling should be performed. This mask is very handful to guide the sampling process, for instance to remove background voxels, deal with class imbalance etc...\n","\n","For our example, let's generate a ROI mask for each subject simply with the binary brain mask:"]},{"cell_type":"code","metadata":{"id":"z4ZeHKboHu1d","colab_type":"code","colab":{}},"source":["options = {}\n","\n","# training data path\n","options['training_path'] = 'data/Training_Set'\n","\n","# validation data path\n","options['validation_path'] = 'data/Validation_Set'\n","\n","# test data path\n","options['test_path'] = 'data/Test_Set'\n","\n","# train/validation split percentage\n","options['train_split'] = 0.3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAdDdJ5sy5Af","colab_type":"code","colab":{}},"source":["# -----------------------------\n","# create image roi masks\n","# -----------------------------\n","\n","def mask_image(im):\n","  return (im > 0).astype('float32')\n","\n","\n","for scan_id in os.listdir(options['training_path']):\n","  scan = ants.image_read(os.path.join(options['training_path'], scan_id, '{}.nii.gz'.format(scan_id)))\n","  brainmask = ants.image_clone(scan).apply(mask_image)\n","  brainmask.to_filename(os.path.join(options['training_path'], scan_id, '{}_brainmask.nii.gz'.format(scan_id)))\n","\n","for scan_id in os.listdir(options['validation_path']):\n","  scan = ants.image_read(os.path.join(options['validation_path'], scan_id, '{}.nii.gz'.format(scan_id)))\n","  brainmask = ants.image_clone(scan).apply(mask_image)\n","  brainmask.to_filename(os.path.join(options['validation_path'], scan_id, '{}_brainmask.nii.gz'.format(scan_id)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyuOgxc-c-qF","colab_type":"text"},"source":["Now, we can define the input dictionaries for training and validation: "]},{"cell_type":"code","metadata":{"id":"mRJECUg5eJg7","colab_type":"code","colab":{}},"source":["# -----------------------------\n","# load data\n","# -----------------------------\n","\n","training_data = os.listdir(options['training_path'])\n","validation_data = os.listdir(options['validation_path'])\n","\n","input_train_data = {scan: [os.path.join(options['training_path'], scan, '{}.nii.gz'.format(scan))]\n","                    for scan in training_data}\n","\n","input_train_labels = {scan: [os.path.join(options['training_path'], scan, '{}_seg.nii.gz'.format(scan))]\n","                      for scan in training_data}\n","\n","input_train_rois = {scan: [os.path.join(options['training_path'], scan, '{}_brainmask.nii.gz'.format(scan))]\n","                    for scan in training_data}\n","\n","input_val_data = {scan: [os.path.join(options['validation_path'], scan, '{}.nii.gz'.format(scan))]\n","                  for scan in validation_data}\n","\n","input_val_labels = {scan: [os.path.join(options['validation_path'], scan, '{}_seg.nii.gz'.format(scan))]\n","                    for scan in validation_data}\n","\n","input_val_rois = {scan: [os.path.join(options['validation_path'], scan, '{}_brainmask.nii.gz'.format(scan))]\n","                  for scan in validation_data}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7mbGYUk3c-qI","colab_type":"text"},"source":["Build the `Datasets` for training and validation data. Here, we need to define the patch size (in 3D), and how we are going to sample the images. We [normalize](https://datascience.stackexchange.com/questions/32109/zero-mean-and-unit-variance) images with `mean=0` and `std=1`. In particular, we are going to use:\n","\n","* Patch size of `32x32x32`\n","* Sampling step of `8x8x8`\n","* Batch size of `128`\n","\n"]},{"cell_type":"code","metadata":{"id":"wLhIxiBcc-qF","colab_type":"code","outputId":"4f214956-27ad-4c35-8df2-27df0a4b27b7","executionInfo":{"status":"ok","timestamp":1578715525773,"user_tz":-60,"elapsed":42087,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":354}},"source":["# -----------------------------\n","# prepare patches and data loaders\n","# -----------------------------\n","\n","# additional options for patch size, sampling step, normalization, etc...\n","options['patch_size'] = (32,32,32)\n","options['sampling_step'] = (16,16,16)\n","options['normalize'] = True\n","options['batch_size'] = 128\n","\n","print('Training data: ')\n","training_dataset = MRI_DataPatchLoader(input_data=input_train_data,\n","                                       labels=input_train_labels,\n","                                       rois=input_train_rois,\n","                                       patch_size=options['patch_size'],\n","                                       sampling_step=options['sampling_step'],\n","                                       normalize=options['normalize'])\n","\n","training_dataloader = DataLoader(training_dataset,\n","                                 batch_size=options['batch_size'],\n","                                 shuffle=True)\n","\n","print('Validation data: ')\n","validation_dataset = MRI_DataPatchLoader(input_data=input_val_data,\n","                                        labels=input_val_labels,\n","                                        rois=input_val_rois,\n","                                        patch_size=options['patch_size'],\n","                                        sampling_step=options['sampling_step'],\n","                                        normalize=options['normalize'])\n","\n","validation_dataloader = DataLoader(validation_dataset,\n","                                   batch_size=options['batch_size'],\n","                                   shuffle=True)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training data: \n","> DATA: Loaded scan IBSR_16 roi size: 1460808 label_size:  1385945\n","> DATA: Loaded scan IBSR_08 roi size: 868943 label_size:  787260\n","> DATA: Loaded scan IBSR_06 roi size: 1078395 label_size:  977824\n","> DATA: Loaded scan IBSR_18 roi size: 1668281 label_size:  1590195\n","> DATA: Loaded scan IBSR_04 roi size: 1069342 label_size:  1027700\n","> DATA: Loaded scan IBSR_03 roi size: 951358 label_size:  899576\n","> DATA: Loaded scan IBSR_05 roi size: 1040178 label_size:  950111\n","> DATA: Loaded scan IBSR_07 roi size: 867764 label_size:  799514\n","> DATA: Loaded scan IBSR_01 roi size: 1147470 label_size:  1093813\n","> DATA: Loaded scan IBSR_09 roi size: 1002429 label_size:  927415\n","> DATA: Training sample size: 2745\n","Validation data: \n","> DATA: Loaded scan IBSR_11 roi size: 958119 label_size:  889996\n","> DATA: Loaded scan IBSR_17 roi size: 1565803 label_size:  1502254\n","> DATA: Loaded scan IBSR_14 roi size: 1140358 label_size:  1078981\n","> DATA: Loaded scan IBSR_13 roi size: 1058473 label_size:  1022741\n","> DATA: Loaded scan IBSR_12 roi size: 954168 label_size:  861321\n","> DATA: Training sample size: 1385\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TMbbN_p6et8z","colab_type":"code","colab":{}},"source":["def dice_loss(input, target):\n","  smooth = 1.\n","  total_loss =0\n","  n_classes = 4\n","  for c in range(n_classes):\n","          tflat = torch.flatten(target ==c)\n","          iflat = torch.flatten(input[:,c,:,:,:])\n","\n","\n","          intersection = torch.sum(iflat*tflat)\n","          #intersection = (iflat * tflat).sum()\n","          #print('intersection: ', intersection)\n","          a = (2. * intersection + smooth)\n","          #b = (iflat.sum() + tflat.sum() + smooth)\n","          \n","          #print('a: ',a,' b: ',b)\n","          loss = ( ((2. * intersection + smooth) /\n","                            (torch.sum(iflat) + torch.sum(tflat) + smooth)))\n","          #print(loss)\n","          if c ==0:\n","            total_loss = loss\n","          else:\n","            total_loss = total_loss + loss\n","  total_loss = total_loss / n_classes\n","\n","  return 1-total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ArdJlGadc-qL","colab_type":"text"},"source":["# 3. Training the network:\n","\n","---\n","\n","\n","Finally we can build our training loop to optimize the `Unet` model based on the same principles we covered in the first part of this tutorial. Basically, what we are going to do is to iterate the training dataset for a number of epochs, saving the weights of the model accordingly. \n","\n","For this first example, we are going to use the [cross-entropy](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss) loss and the [Adadelta](https://pytorch.org/docs/stable/optim.html#torch.optim.Adadelta) optimizer:\n","\n"]},{"cell_type":"code","metadata":{"id":"j2EMNzRjc-qL","colab_type":"code","outputId":"731ffe03-d9b0-4eec-8eed-d11631b40fb8","executionInfo":{"status":"ok","timestamp":1578723197327,"user_tz":-60,"elapsed":3139641,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# -----------------------------\n","# Model training\n","# -----------------------------\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","# Define the Unet model\n","# 2 input channels (FLAIR and T1)\n","# 2 output classes (healthy and wmh) (we ignore other pathologies)\n","# lesion_model = Unet(input_size=1, output_size=4)\n","lesion_model = ResUNet(outputsize=4, inputsize=1, k=16)\n","\n","model_name = 'test_maia_wmh'\n","\n","# some training options\n","options['gpu_use'] = True\n","options['num_epochs'] = 100\n","dice = False\n","\n","# define the torch.device\n","device = torch.device('cuda') if options['gpu_use'] else torch.device('cpu')\n","\n","# define the optimizer\n","optimizer = Adam(lesion_model.parameters())\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n","\n","# send the model to the device\n","lesion_model = lesion_model.to(device)\n","\n","# initialize accuracy and loss lists\n","train_loss_all = []\n","train_acc_all = []\n","val_loss_all = []\n","val_acc_all = []\n","\n","# initialize best accuracy and loss\n","best_acc = 0.0\n","best_loss = 1e+5\n","\n","# initialize the early_stopping counter\n","early_count = 0\n","\n","# training loop\n","training = True\n","epoch = 1\n","try:\n","    since = time.time()\n","    while training:\n","\n","        # epoch specific metrics\n","        train_loss = 0\n","        train_accuracy = 0\n","        val_loss = 0\n","        val_accuracy = 0\n","\n","        # -----------------------------\n","        # training samples\n","        # -----------------------------\n","\n","        # set the model into train mode\n","        lesion_model.train()\n","        for b, batch in enumerate(Bar(training_dataloader)):\n","            # process batches: each batch is composed by training (x) and labels (y)\n","            # x = [32, 2, 32, 32, 32]\n","            # y = [32, 1, 32, 32, 32]\n","\n","            x = batch[0].to(device)\n","            y = batch[1].to(device)\n","\n","            # clear gradients\n","            optimizer.zero_grad()\n","\n","            # infer the current batch\n","            pred = lesion_model(x)\n","\n","            # compute the loss.\n","            # we ignore the index=2\n","            if dice:\n","              #print(pred.shape)\n","              #print(y.shape)\n","              loss = dice_loss(pred,y)\n","              train_loss += loss.item()\n","            else:\n","              loss = F.cross_entropy(torch.log(torch.clamp(pred, 1E-7, 1.0)),\n","                                    y.squeeze(dim=1).long())\n","              train_loss += loss.item()\n","\n","            # backward loss and next step\n","            loss.backward()\n","            optimizer.step()\n","\n","            # compute the accuracy\n","            pred = pred.max(1, keepdim=True)[1]\n","            batch_accuracy = pred.eq(y.view_as(pred).long())\n","            train_accuracy += (batch_accuracy.sum().item() / np.prod(y.shape))\n","\n","        # -----------------------------\n","        # validation samples\n","        # -----------------------------\n","\n","        # set the model into train mode\n","        lesion_model.eval()\n","        for a, batch in enumerate(Bar(validation_dataloader)):\n","            x = batch[0].to(device)\n","            y = batch[1].to(device)\n","\n","            # infer the current batch\n","            with torch.no_grad():\n","                pred = lesion_model(x)\n","\n","                # compute the loss.\n","                # we ignore the index=2\n","                if dice:\n","                  loss = dice_loss(pred, y)\n","                  train_loss += loss.item()\n","                else:\n","                  loss = F.cross_entropy(torch.log(torch.clamp(pred, 1E-7, 1.0)),\n","                                    y.squeeze(dim=1).long())\n","                  val_loss += loss.item()\n","\n","                # compute the accuracy\n","                pred = pred.max(1, keepdim=True)[1]\n","                batch_accuracy = pred.eq(y.view_as(pred).long())\n","                val_accuracy += batch_accuracy.sum().item() / np.prod(y.shape)\n","\n","        # compute mean metrics\n","        train_loss /= (b + 1)\n","        train_accuracy /= (b + 1)\n","        val_loss /= (a + 1)\n","        val_accuracy /= (a + 1)\n","\n","        train_loss_all.append(train_loss)\n","        train_acc_all.append(train_accuracy)\n","        val_loss_all.append(val_loss)\n","        val_acc_all.append(val_accuracy)\n","\n","\n","        print('Epoch {:d} train_loss {:.4f} train_acc {:.4f} val_loss {:.4f} val_acc {:.4f}'.format(\n","            epoch,\n","            train_loss,\n","            train_accuracy,\n","            val_loss,\n","            val_accuracy))\n","        \n","        if val_loss < best_loss:\n","              # save weights\n","              best_loss = val_loss\n","              best_acc = val_accuracy\n","              print(\"val loss decreased...saving model\")\n","              best_model_wts = copy.deepcopy(lesion_model.state_dict()) #copy its weights\n","              model_path = \"{}/model.pt\".format(tmpdir)\n","              torch.save(lesion_model.state_dict(),model_path)\n","              early_count = 0\n","        else:\n","              early_count += 1\n","\n","        # update epochs and lr\n","        epoch += 1\n","        scheduler.step(val_loss)\n","    \n","        if early_count == 10:\n","            print(\"Early stopping\")\n","            training = False\n","\n","        if epoch >= options['num_epochs']:\n","            training = False\n","\n","    # load best model weights\n","    lesion_model.load_state_dict(best_model_wts)\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60)) \n","\n","except KeyboardInterrupt:\n","    pass"],"execution_count":15,"outputs":[{"output_type":"stream","text":["2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 1 train_loss 0.9991 train_acc 0.6751 val_loss 0.7483 val_acc 0.8069\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 2 train_loss 0.6943 train_acc 0.8202 val_loss 0.5698 val_acc 0.8586\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 3 train_loss 0.5597 train_acc 0.8524 val_loss 0.4746 val_acc 0.8767\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 4 train_loss 0.4796 train_acc 0.8662 val_loss 0.4101 val_acc 0.8869\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 5 train_loss 0.4272 train_acc 0.8749 val_loss 0.3689 val_acc 0.8920\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 6 train_loss 0.3879 train_acc 0.8822 val_loss 0.3273 val_acc 0.9015\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 7 train_loss 0.3521 train_acc 0.8914 val_loss 0.3018 val_acc 0.9082\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 8 train_loss 0.3251 train_acc 0.8988 val_loss 0.2804 val_acc 0.9156\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 9 train_loss 0.3066 train_acc 0.9033 val_loss 0.2637 val_acc 0.9193\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 10 train_loss 0.2869 train_acc 0.9090 val_loss 0.2486 val_acc 0.9235\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 11 train_loss 0.2721 train_acc 0.9129 val_loss 0.2322 val_acc 0.9287\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 12 train_loss 0.2566 train_acc 0.9177 val_loss 0.2215 val_acc 0.9300\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 13 train_loss 0.2467 train_acc 0.9199 val_loss 0.2289 val_acc 0.9265\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 14 train_loss 0.2420 train_acc 0.9202 val_loss 0.2065 val_acc 0.9332\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 15 train_loss 0.2322 train_acc 0.9226 val_loss 0.2071 val_acc 0.9322\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 16 train_loss 0.2219 train_acc 0.9257 val_loss 0.1913 val_acc 0.9380\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 17 train_loss 0.2125 train_acc 0.9289 val_loss 0.1880 val_acc 0.9380\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 18 train_loss 0.2072 train_acc 0.9300 val_loss 0.1844 val_acc 0.9385\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 19 train_loss 0.2037 train_acc 0.9305 val_loss 0.1945 val_acc 0.9328\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 20 train_loss 0.2009 train_acc 0.9307 val_loss 0.1753 val_acc 0.9403\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 21 train_loss 0.1914 train_acc 0.9337 val_loss 0.1673 val_acc 0.9431\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 22 train_loss 0.1862 train_acc 0.9353 val_loss 0.1648 val_acc 0.9432\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 23 train_loss 0.1832 train_acc 0.9360 val_loss 0.1632 val_acc 0.9433\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 24 train_loss 0.1777 train_acc 0.9376 val_loss 0.1634 val_acc 0.9425\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 25 train_loss 0.1755 train_acc 0.9379 val_loss 0.1583 val_acc 0.9446\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 26 train_loss 0.1714 train_acc 0.9392 val_loss 0.1563 val_acc 0.9444\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 27 train_loss 0.1680 train_acc 0.9400 val_loss 0.1554 val_acc 0.9442\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 28 train_loss 0.1671 train_acc 0.9401 val_loss 0.1541 val_acc 0.9452\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 29 train_loss 0.1637 train_acc 0.9410 val_loss 0.1529 val_acc 0.9450\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 30 train_loss 0.1612 train_acc 0.9417 val_loss 0.1497 val_acc 0.9464\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 31 train_loss 0.1651 train_acc 0.9398 val_loss 0.1512 val_acc 0.9456\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 32 train_loss 0.1617 train_acc 0.9409 val_loss 0.1543 val_acc 0.9425\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 33 train_loss 0.1576 train_acc 0.9422 val_loss 0.1469 val_acc 0.9470\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 34 train_loss 0.1551 train_acc 0.9432 val_loss 0.1461 val_acc 0.9463\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 35 train_loss 0.1508 train_acc 0.9445 val_loss 0.1421 val_acc 0.9478\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 36 train_loss 0.1472 train_acc 0.9458 val_loss 0.1412 val_acc 0.9479\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 37 train_loss 0.1460 train_acc 0.9459 val_loss 0.1433 val_acc 0.9470\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 38 train_loss 0.1454 train_acc 0.9461 val_loss 0.1382 val_acc 0.9489\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 39 train_loss 0.1427 train_acc 0.9471 val_loss 0.1389 val_acc 0.9483\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 40 train_loss 0.1410 train_acc 0.9474 val_loss 0.1389 val_acc 0.9479\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 41 train_loss 0.1391 train_acc 0.9481 val_loss 0.1414 val_acc 0.9467\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 42 train_loss 0.1394 train_acc 0.9478 val_loss 0.1405 val_acc 0.9467\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 43 train_loss 0.1370 train_acc 0.9484 val_loss 0.1361 val_acc 0.9491\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 44 train_loss 0.1364 train_acc 0.9488 val_loss 0.1371 val_acc 0.9483\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 45 train_loss 0.1350 train_acc 0.9490 val_loss 0.1361 val_acc 0.9484\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 46 train_loss 0.1342 train_acc 0.9493 val_loss 0.1343 val_acc 0.9494\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 47 train_loss 0.1312 train_acc 0.9505 val_loss 0.1424 val_acc 0.9464\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 48 train_loss 0.1341 train_acc 0.9490 val_loss 0.1341 val_acc 0.9494\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 49 train_loss 0.1299 train_acc 0.9507 val_loss 0.1347 val_acc 0.9487\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 50 train_loss 0.1279 train_acc 0.9513 val_loss 0.1365 val_acc 0.9476\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 51 train_loss 0.1263 train_acc 0.9519 val_loss 0.1324 val_acc 0.9499\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 52 train_loss 0.1258 train_acc 0.9521 val_loss 0.1320 val_acc 0.9498\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 53 train_loss 0.1247 train_acc 0.9523 val_loss 0.1330 val_acc 0.9489\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 54 train_loss 0.1242 train_acc 0.9525 val_loss 0.1314 val_acc 0.9501\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 55 train_loss 0.1228 train_acc 0.9528 val_loss 0.1324 val_acc 0.9494\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 56 train_loss 0.1215 train_acc 0.9533 val_loss 0.1315 val_acc 0.9500\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 57 train_loss 0.1251 train_acc 0.9518 val_loss 0.1356 val_acc 0.9477\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 58 train_loss 0.1232 train_acc 0.9525 val_loss 0.1320 val_acc 0.9496\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 59 train_loss 0.1212 train_acc 0.9532 val_loss 0.1305 val_acc 0.9503\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 60 train_loss 0.1191 train_acc 0.9540 val_loss 0.1362 val_acc 0.9476\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 61 train_loss 0.1181 train_acc 0.9544 val_loss 0.1316 val_acc 0.9498\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 62 train_loss 0.1188 train_acc 0.9539 val_loss 0.1322 val_acc 0.9491\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 63 train_loss 0.1168 train_acc 0.9547 val_loss 0.1338 val_acc 0.9486\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 64 train_loss 0.1153 train_acc 0.9553 val_loss 0.1319 val_acc 0.9496\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 65 train_loss 0.1150 train_acc 0.9553 val_loss 0.1359 val_acc 0.9478\n","Epoch    64: reducing learning rate of group 0 to 5.0000e-04.\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 66 train_loss 0.1123 train_acc 0.9564 val_loss 0.1286 val_acc 0.9511\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 67 train_loss 0.1096 train_acc 0.9576 val_loss 0.1294 val_acc 0.9508\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 68 train_loss 0.1093 train_acc 0.9575 val_loss 0.1273 val_acc 0.9514\n","val loss decreased...saving model\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 69 train_loss 0.1089 train_acc 0.9577 val_loss 0.1299 val_acc 0.9504\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 70 train_loss 0.1089 train_acc 0.9577 val_loss 0.1301 val_acc 0.9503\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 71 train_loss 0.1090 train_acc 0.9576 val_loss 0.1291 val_acc 0.9508\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 72 train_loss 0.1080 train_acc 0.9580 val_loss 0.1278 val_acc 0.9514\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 73 train_loss 0.1090 train_acc 0.9576 val_loss 0.1288 val_acc 0.9507\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 74 train_loss 0.1076 train_acc 0.9581 val_loss 0.1288 val_acc 0.9510\n","Epoch    73: reducing learning rate of group 0 to 2.5000e-04.\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 75 train_loss 0.1061 train_acc 0.9588 val_loss 0.1293 val_acc 0.9508\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 76 train_loss 0.1054 train_acc 0.9590 val_loss 0.1275 val_acc 0.9513\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 77 train_loss 0.1047 train_acc 0.9593 val_loss 0.1283 val_acc 0.9514\n","2745/2745: [==============================>.] - ETA 1.6s\n","1385/1385: [=============================>..] - ETA 0.6s\n","Epoch 78 train_loss 0.1046 train_acc 0.9592 val_loss 0.1295 val_acc 0.9508\n","Early stopping\n","Training complete in 52m 19s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JlqgvRLkZdcO","colab_type":"code","outputId":"b8470103-9c80-4311-c41c-a12e4b30a8df","executionInfo":{"status":"ok","timestamp":1578723197734,"user_tz":-60,"elapsed":2800298,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["# training and val accuracy\n","plt.figure()\n","plt.plot(train_acc_all)\n","plt.plot(val_acc_all)\n","plt.title('Accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epochs')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(\"{}/accuracy.png\".format(tmpdir))\n","\n","# training and val loss\n","plt.figure()\n","plt.plot(train_loss_all)\n","plt.plot(val_loss_all)\n","plt.title('Loss')\n","plt.ylabel('loss')\n","plt.xlabel('epochs')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.savefig(\"{}/loss.png\".format(tmpdir))"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3xc1Zn/8c+jUS+WVdy7DbjQDBjT\nExJCqKElS2gJaevd34Ysm2STkF1CCNkkZDcFyJLKkoTQQknxBi/dkIBpBgwYF1xwkW3ZclGzNNKU\n5/fHvbLHsmSNjUcztr7v12temlvnmaL73HvOPeeYuyMiIrInedkOQEREcp+ShYiI9EnJQkRE+qRk\nISIifVKyEBGRPilZiIhIn5QsRESkT0oWMuCZ2TNmts3MirIdi0iuUrKQAc3MxgOnAQ5c0I+vm99f\nryWyPyhZyED3SeBF4DfA1V0zzazEzH5oZqvNrMnMnjOzknDZqWY2z8wazWytmX0qnP+MmX0uZR+f\nMrPnUqbdzD5vZsuAZeG8W8N9NJvZq2Z2Wsr6ETP7NzNbYWYt4fIxZna7mf0w9U2Y2Wwz+2ImPiAR\nULIQ+SRwT/g4y8yGhfN/ABwHnAxUA18FkmY2Dvg/4CfAEGA6sGAvXu8i4ARgWjj9SriPauBe4EEz\nKw6XfQm4HDgXGAR8BmgDfgtcbmZ5AGZWC3wo3F4kI5QsZMAys1OBccAD7v4qsAK4IjwIfwa41t3X\nuXvC3ee5ewdwBfCku9/n7jF33+Lue5MsvufuW929HcDd7w73EXf3HwJFwORw3c8B17v7Ug+8Ea77\nMtAEnBGudxnwjLtvfI8fiUivlCxkILsaeNzdN4fT94bzaoFiguTR3Zhe5qdrbeqEmf2rmS0Oi7oa\ngcrw9ft6rd8CV4XPrwJ+9x5iEumTKtlkQArrHy4FImZWH84uAgYDI4AoMAl4o9uma4GZvex2O1Ca\nMj28h3V2dPMc1k98leAK4W13T5rZNsBSXmsSsLCH/dwNLDSzo4GpwJ96iUlkv9CVhQxUFwEJgrqD\n6eFjKvA3gnqMO4EfmdnIsKL5pPDW2nuAD5nZpWaWb2Y1ZjY93OcC4BIzKzWzQ4DP9hFDBRAHGoB8\nM7uBoG6iyx3At83sUAscZWY1AO5eR1Df8Tvg4a5iLZFMUbKQgepq4Nfuvsbd67sewH8DVwLXAW8R\nHJC3At8H8tx9DUGF85fD+QuAo8N9/hjoBDYSFBPd00cMjwGPAu8AqwmuZlKLqX4EPAA8DjQD/wOU\npCz/LXAkKoKSfmAa/EjkwGRm7yMojhrn+keWDNOVhcgByMwKgGuBO5QopD8oWYgcYMxsKtBIUBF/\nS5bDkQFCxVAiItInXVmIiEifDpp2FrW1tT5+/PhshyEickB59dVXN7v7kL7WO2iSxfjx45k/f362\nwxAROaCY2ep01lMxlIiI9EnJQkRE+qRkISIifTpo6ix6EovFqKurIxqNZjuUjCsuLmb06NEUFBRk\nOxQROQgd1Mmirq6OiooKxo8fj5n1vcEByt3ZsmULdXV1TJgwIdvhiMhB6KAuhopGo9TU1BzUiQLA\nzKipqRkQV1Aikh0HdbIADvpE0WWgvE8RyY6DuhhKRORA4u40tcfY3NpBNJakI56kI5agrTNBS0eM\nlmiclmicjniS/DwjEj5GVBZz4fRRGY1NySLDGhsbuffee/mnf/qnvdru3HPP5d5772Xw4MEZikxE\nelLfFGVxfTOJhJOXB3lmmBmd8SQd8QTRWJLOeBJn1371goN3Hvl5wVV+SzRGczROc3uMjniSQcX5\nVJYWMrikgML8PLZu72RLawebt3eyqbmDum1t1G1rp7UjvtcxHzN2sJLFga6xsZGf/vSnuyWLeDxO\nfn7vH/+cOXMyHZrIgObuNLR08PaGZhatb+aNtY28UdfIxuaO/fo6Rfl5FOXn0dIRp3u/rWZQXVrI\nkIoiRleVcOLEGkZXlTCkooiSgghFBRGK8/MoKYxQUVxARXE+FcX5FEbySDrEk0kSSd9tv5mgZJFh\n1113HStWrGD69OkUFBRQXFxMVVUVS5Ys4Z133uGiiy5i7dq1RKNRrr32WmbNmgXs7L6ktbWVc845\nh1NPPZV58+YxatQo/vznP1NSUtLHK4scnBJJp7Gtk8L8PMqL8nepr4vGEtRta2PN1jbWN0apb4qy\noSnKppYosUQSAMNIJJ0VDa1s2d65Y9uJtWWcPKmWo0dXcvioSorzIyTcSbrj7hTlR8IDf4TC/Dzy\nUqoJPYwrnnDiySQODAoP7sUFEQCSSaclGqexvZOOeJLqskKqSguJ5O1bfWPEIJIX2adt98WASRbf\n+t+3WbS+eb/uc9rIQXzzI4fvcZ2bb76ZhQsXsmDBAp555hnOO+88Fi5cuOMW1zvvvJPq6mra29s5\n/vjj+ehHP0pNTc0u+1i2bBn33Xcfv/rVr7j00kt5+OGHueqqq/brexHZn9yd9liCLa2dbN0ePMqK\n8jl6TCVF+b0f4BLJ4Gx/fVM7a7e2sXZrG6u3BAf/hpYOtrZ10tQe23EmXRjJo7qskMGlBTS2xahv\n3vWOwEieMbSiiKGDiinKzwuO6jgYnDF1KNNGDGLayEqmjKhgUHFm2yjl5RmVpQVUlh6YbaEymizM\n7GzgViBCMKLXzd2WjwPuBIYQjGd8VTgQPWaWIBgDGWCNu1+QyVj7y8yZM3dpC3Hbbbfxxz/+EYC1\na9eybNmy3ZLFhAkTmD59OgDHHXccq1at6rd4ZeBIJp0t2zvZ1BINDszbO2mPJWjvTNART9LemaA9\nrGyNhvOj8QQdsSTReDDd2hGntSPO9o44scTuZSPFBXkcP76akybVMKS8iLpt7awNy+rXbWtnY3OU\neHLX7YZWFDG2upSpIwdRXVoYnpEX0JlIsmV7J1tbO9nW1sm0EYMYW1PKuJpSxlaXMbqqhNryon0+\nc+/24UDePt486g6ehPd6FeAOyQQkY5BXAJH+PdfP2KuZWQS4HTgTqANeMbPZ7r4oZbUfAHe5+2/N\n7IPA94BPhMva3X36/oqnryuA/lJWVrbj+TPPPMOTTz7JCy+8QGlpKaeffnqPbSWKiop2PI9EIrS3\nt/dLrHJgiieSrGjYzrubt3PEqEGMrirdZXk0luC5ZZt5ceUW1je1s6EpKK7Z1NJBItl74XeeQWlh\nUKxSWhihuCCP4oIIxfkRKgthTGmS/JJqyovzKSvKZ1BxATVlwcG9utTY2hrj+Xcbmbd8C//56FIg\nKLMfMaiY0VWlzJxQzYhBRYwvizIhspmqEZMYNXocJYUZLmpZ9iS88JPgQFw0CIoqoKAYWhugaS00\nr4O2rVA1HoYdDsOOgCGHQUFpcNDOiwAerNO2BbZvhu2boKkOmtYFfztbYdAoqBoX7Ke0JpgXbYZo\nEyQ6IL84eBSUBMmldRNsbwj+djRDIgZdleqWB+XDoXJUsN+R0+HUL2b0Y8pkapoJLHf3lQBmdj9w\nIZCaLKYBXwqfzwX+lMF4sqKiooKWlpYelzU1NVFVVUVpaSlLlizhxRdf7OfoZL9xh41vQ0dL8A9c\nPhzyC/d9f51tsPwJPFLE9vKxbC0YwbYO23FWHzziJJvWUbLtHcqallHYtoHZiZP5U8NI2mOJHbsa\nW13KFSPW8f7oM6zYXsS8rWW8G69lQ94I8gaPZsTgEk6eVMvwyiKGl+czrfUlJtU9zKD6l4Kz6Ugh\nFimAwnKsZhLUHAI1k4ID2/oFsP51qH8T4lEoqoTBY2DwWIgUBgfapnXQsgFKqvjQsZ+AT32GzQUj\naI3GGVmeR+GqubDo97DhDVi+BmJtQeCRQph+BZxyLVRPDOZt3wxvPQhvPhCcYQ8eF7xW5RioGA5l\nQ6CsFkqqoWU9bFwEmxbBlhUwdApMOR9GHhu8r83L4bF/g2WPQeXY4HtrXBMcmGNtwb4GjYJRxwb7\n27oi+I6XPALsqUbZgmRQOTr4nCa8L0hATXXQuBpWzA2SSlEFFA8KElR+cZA44lGItQdZtGwoVE2A\nMTOhuDL4PCKFkJcfrNMcJqKNC4N4D+BkMQpYmzJdB5zQbZ03gEsIiqouBirMrMbdtwDFZjYfiAM3\nu/tuicTMZgGzAMaOHbv/38F+UFNTwymnnMIRRxxBSUkJw4YN27Hs7LPP5uc//zlTp05l8uTJnHji\niVmMVHbRvB5WPQfbVkOic+ejqAJqD4PaQ4OD5rZVsPAP8PYfg4PJDoaXD8VLa4kXDqIzv4KO/HK2\nFI1lZfE03vKJrGoNzphLu87UCyMUNL7LkRse5uSWR6nwVgwoB0rdiFBN0vMotBiFxCmhg2KL7XjF\nGBFO4kGurj6Ndcd+icHjj6Fu0YtMfPO7HLniZdq8iMMsxvl5SejKY7FKiBwB5UdAXjG8+EBwYC8f\nBkdfGhycErHgwBxtCg66K5+FeHh1W1AKI46GGZ+F8qHBAaxxbfC5xaPBAXPSB4MD8abFMO+/4fnb\nqD30w9SWVgcH3o5mKKmCcafApDPCg/8oWP4ULLgHXrsLpl0UfP7vPBbEMmI6VIwM4lkxF2Lbe/8u\nI0XB2fyyx+G5HweJfPQMeOdRyC+BM2+CE/4R8ot630eqzjbY9i7EO8JioTjgQUIpqw3eSz9WPPeX\njI3BbWYfA85298+F058ATnD3a1LWGQn8NzAB+CvwUeAId280s1Huvs7MJgJPA2e4+4rdXig0Y8YM\n7z740eLFi5k6der+fms5a6C93120boLVz8PWd4MDVNX44KyzfGhwltZdrD08q3wSKoYFZ5CDRgVF\nA6v+BltXpqxskF+E5+VDrA3z5C67csujcdiJbBxzDnXJGpo3riG+bS2R7RuoSDQxyNoYRBuV1soo\n2wJA0o1VkbE0WQWedJKepCjZwZF5K4kT4dXSU3lj6EWUlpUzKlnPsMQGqjrXE8nLIy+/kEhBEXkF\nxVjtJAqHT6NoxDQsvwhe+hk8/5PgADzqOFg3H4oHkzjlX6if/AlGVg/CmtcFZ9BbV0D9Qqh/Kzhj\njrXBoWfCsVfDYWdBpJeK2GQySAqxtiBh7s2BsWkdvPqb4BHvgKnnw+GXwMT39/x6LfXwwu0w/86g\neOaoj8P0K2HYtJQvICwCat0IbZuDopu2rUHCGzoNqicEMbZvg2VPBAlq9bzgPZ5xQ/AbGcDM7FV3\nn9HnehlMFicBN7r7WeH01wHc/Xu9rF8OLHH30T0s+w3wF3d/qLfXU7I4AN9vvAPWvhyUz+YXhY/i\n4Gy1sCx4YLD5naAoYePbwUEuUhiUKeeXBAesNS/ClmU9v0b5MHjfV+C4T+08GG19Fx74ZFB0Mvm8\nIEF0FZdECmHcyTDhNDZWz+DF1mHMX9vKq6u3saS+mQLvZJxtZJKtZ5Ktp5FyHk3MZDOVO16yojif\nKcMrmDy8ghGVJQwqKWBQeH/88MIoY9oWUd7wOrbuteC1LS94n3l5MO5UOO7qoEhlX7VthXk/gUV/\nhsMvhpO/ACV9NO5MJoOz86KKfX/dvZFMAp5+ool3Bp9TP1fqDgS5kCzygXeAM4B1wCvAFe7+dso6\ntcBWd0+a2XeAhLvfYGZVQJu7d4TrvABc2K1yfBdKFll6vxveDM7k8J23JXZdmifjQSKoGBGc6ZcP\nC87y170WFC8sfBiijem/VqQwKKJIxiEWDYpCLAKjj4fxpwTFGLWHBcUo21YFRSGL/hRccVRPhA9+\nI0hEfwzasnDJr4KzS6C9M8GKTS28ta6Jl1dt4+V3t7KuMShqKSuMcMzYKo4dV8X4mlIKInkURIz8\nvDwK8oPnBZGg5e6wQcWMqCxWX11ywEg3WWQsTbt73MyuAR4juHX2Tnd/28xuAua7+2zgdOB7ZuYE\nxVCfDzefCvzCzJIEnR3evKdEIf3MHd79Kzz7/eBAnK784qCirnVj8HzK+XDkx4IKvng0KJOOtQdX\nC51twZluIh5UEg6dFvztoajC3WmOxtnQ1E58izNtxGHkDZkcLJz590FZ9ZM3wkOfBiA57EheO/E2\n/rq6nLeef5nlDa3UbWvfce9+bXkRJ0yoZtb7JjJjfBVThg/aP7dfihzAMnpN5+5zgDnd5t2Q8vwh\nYLeiJXefBxyZydhkH737N3j6P2Dti8EVw9k3w8hjAAvrBiwoWsjLDx4Q3JWybVXwaKmH8afB4RcF\niWMPEklnc2vHjvvuvTlGe2eUZZtaWVLfwtL6ZlY0bGdDYzvbO3fe/TNsUBHnHDGC844awZGjKlle\ndiKLj7+P/IUPEmtYxrfWnsv21RvIMzhsWAXTx1TxsWPHcMjQcqaOqGBCbZmuDES6UQGg7CoR771c\neNXzcNeFQXn6uT+AYz4R1B30JbUyshcbmtp5ZdU2Xl+zjVWbt7N6Sxtrt7X12LALgnv+x9eUccjQ\nck47tJaRlSWMGFxMZzzJowvrufflNfxm3qpdtiktPJLDR57Cp46uZuaEGo4dO5iKDLfaFTlYKFkM\nJO4w77bgltDUeZ2tQb3D9obg9sipF8BH79j1VsLWBnjoM0GjolnPBveH76X1je289O4WtrR27uiN\nc3NrB6+vadxRP1BSEGFCbRlTRlRw1hHDGTW4hIKIYQRn+oX5eRwytJxDhpbv6HOnu0uOHU1rR5yn\nFm9kxaZWDhtewbQRgxhfU0aeipNE9omSRYbtaxflALfccguzZs2itLS075X74g5PfhOevxWGTAlu\nQ+xSWA7DjwwaISUTMP9/4IGr4dLfBgkjmYA/fC649fCqh9JKFNFYgg1NUVZt2c685Zt59p0G3tnY\numO5GZQX5TO4tICjx1Ty2VMnMHNCNVOGV5Afee9jcpUX5We8y2aRgUTJIsN666I8HbfccgtXXXXV\ne08W7vD0t4NEcfzngiKkPZXJD5sGj3x5Z8J47hZY+Qx85LYgqXQTSyRZsLaRv73TwLwVW1i1pY3N\nrTu7eS6M5DFzQjV/d9wYTj20llFVJZQX5ussX+QAomSRYaldlJ955pkMHTqUBx54gI6ODi6++GK+\n9a1vsX37di699FLq6upIJBJ84xvfYOPGjaxfv54PfOAD1NbWMnfu3PResHFt0H5g6FQYPD64d/+Z\nm+FvPwwaW53zX3tOFBAkFAwe+RL85jyomw9HXQbHfnKX1ZbUN/OTp5fz7NIGWjvi5BkcNXowZ0wZ\nyqiqEkYOLmHU4BKOHlNJaaF+aiIHsoHzH/x/1wUtVfen4UfCOTfvcZXULsoff/xxHnroIV5++WXc\nnQsuuIC//vWvNDQ0MHLkSB555BEg6DOqsrKSH/3oR8ydO5fa2tr04knG4ZenB61YAQrKgtarGxfC\n9Kvg/FvS7znz+M8Gfx/5UlBsdf6PdiSZdY3t/Ojxd/jD63VUFOXzkaNH8v7DajlpYu0B2/2yiOzZ\nwEkWOeDxxx/n8ccf55hjjgGgtbWVZcuWcdppp/HlL3+Zr33ta5x//vmcdtppve+kt5avyUTQyVoy\nBpf/Puj1cuPbweOka4L+b9JIFK0dceqb2mlo6WRL0TlEjq+iPjKCbc/UEY0n2dLayf++uR6AWadN\n5P+dPonBpe+hwzwROSAMnGTRxxVAf3B3vv71r/MP//APuy177bXXmDNnDtdffz1nnHEGN9wQNkdJ\nJoKK5c7twaOrR8qKkUGnZWZBncS2VUGHb3/3W5j0gbRjmr9qK39btpnFG5pZUt/Cmq1t3dYoIhhq\nZCvFBXmUFES44OiRfPHMwxg1WKP1iQwUAydZZElqF+VnnXUW3/jGN7jyyispLy9n3bp1FBQUEI/H\nqa6u5qqrrmLw4MHccccd4bbltKx+i9pRNUAeFJYGnZ7F2qC5Dtq3Bl0zt28Le+4cDJNmphXXyoZW\nvjtnCU8u3ogZTKgp48hRlVw6YzRjqksZUl5ETXkR1WWFVBTnU5Sfp4ZqIgOYkkWGpXZRfs4553DF\nFVdw0kknAVBeXs7dd9/N8uXL+cpXvkJeXh4FBQX87Gc/g3gHsy67gLMv+xwjR49h7jPPhh3OEVxJ\ntG8LOr/bHAwiQ9kQKOp53IxUTW0xbn1qGXe9sIrigghfPXsyV580nrIi/RREpHcZ60iwvx1UHQnG\n2oN++j0Z9IdUWNbzesk4NG8I1hs8lsVLlvT6fqOxBHe9sIrb566gJRrj48eP4UtnTmZIRZp9+IvI\nQSnrHQnKPupoCbrQtrxggJ2CPdQL5OUHo5LtQTLp/GnBOn74+Dusa2zn/YcN4bpzpjB1xN63wBaR\ngUvJIle4Q2t90NFepCgctnLfz/pbO+L84bU6fjtvFSsatnPkqEr+62NHcfIhad6GKyKS4qBPFu6e\n+xWzic5g7IXO1mBIxsoxaQ8K4+7EEkniCacjnmTu0k08u7SBh1+to6UjztGjK/nJ5cdw3pEj1GJa\nRPbZQZ0siouL2bJlCzU1NbmbMNoboWntjnoHSmvS2qwjlmBrWyfbtseIJRLE25p5bW0L3/nrKgoi\nxvlHjeSTJ43jmLFVGX4DIjIQHNTJYvTo0dTV1dHQ0JDtUHbX1X4i1haMAFdaA42bgE173KwjlqA5\nGqcjnsSA4oI8igsiJCyfKZPG84cjJjO+pozqMjWUE5H956BOFgUFBUyYMCHbYezKHd58AB79WtDI\n7vTrYOY/9zxYfTe/e2EVN/7vIoYPKubKE8fysWNHM3RQGuNJiIi8Rwd1sshJz/4nPPNdGD0TLvxv\n6Br+cw/iiSQ3/WURd72wmg9OGcqtl03XoD0i0q+ULPrT8ifhme/BUR+Hi36WViV2U1uMa+57jb8t\n28ys903ka2dP0XjQItLvlCz6S+NaePjvYei0sPfXvhPF88s385UH36ChtYP//NhRXDpjz20qREQy\nRcmiP8Q74cFPBR39XXpX0MfTHrR3Jvj+o0v4zbxVTBxSxoP/eDLTxwzun1hFRHqgZNEfHr8e1s0P\nEkXtIXtcdfGGZj5/72usbNjOp08Zz1fPmkJJYXptLkREMkXJItPe+D28/As48fMw7cI9rrpofTNX\n3PEiRfl53Pu5E9TaWkRyhpJFJq1+AWZfA+NPgzO/tcdVF29o5so7XqSkIMLvZ53E2Jr3OO62iMh+\nlOYYm7LXtq6E318ZtMq+9K49tqNYUt/MlXe8RFF+hPtnnahEISI5J6PJwszONrOlZrbczK7rYfk4\nM3vKzN40s2fMbHTKsqvNbFn4uDqTce537Y1w78eDLjyueABKq3td9e31TVz5q5coiBj3zzqRcTW9\ndEcuIpJFGUsWZhYBbgfOAaYBl5vZtG6r/QC4y92PAm4CvhduWw18EzgBmAl808wOjE6OEjF44JNB\nN+MfvyfoPbYXs99Yz0d/No+CSB73zzqJ8bVKFCKSmzJ5ZTETWO7uK929E7gf6F7DOw14Onw+N2X5\nWcAT7r7V3bcBTwBnZzDW/ef5W+HdZ+Ejt8L4U3pcJZ5I8t05i/nn+17nyFGVzP7CKUxQohCRHJbJ\nZDEKWJsyXRfOS/UGcEn4/GKgwsxq0twWM5tlZvPNbH5OdBbYuAb++gOYegEcc2WPqzS1xfjUr1/h\nl39dySdOHMc9nzuRoRXq30lEclu2K7j/FXi/mb0OvB9YByTS3djdf+nuM9x9xpAhQzIVY/oe/TqY\nwVnf7XFxIulcc99rvPTuFv7zo0fx7YuOoDA/21+BiEjfMnmkWgek9k8xOpy3g7uvd/dL3P0Y4N/D\neY3pbJtzlj0JS/4C7/vXXoc6/e+nl/O3ZZu58YLDufR4dd0hIgeOTCaLV4BDzWyCmRUClwGzU1cw\ns1oz64rh68Cd4fPHgA+bWVVYsf3hcF5uinfA/30Fag6Bk67pcZW/LWvglqfe4ZJjRnHFzLH9HKCI\nyHuTsWTh7nHgGoKD/GLgAXd/28xuMrMLwtVOB5aa2TvAMOA74bZbgW8TJJxXgJvCeblp3m1Bu4pz\n/rPHcbM3NLVz7f0LOHRoOf9x8RG5O2qfiEgvzN2zHcN+MWPGDJ8/f37/v/DGt+FXZ8BhHw4a33XT\nGU9y2S9fYGl9C7O/cCqThpT3f4wiIr0ws1fdfUZf66l29b1Y9TzceQ6UDO61UvunzyzntTWN3PzR\no5QoROSApWSxrxbNht9dDBXD4LOPQ+Xo3VZZ2dDKT+eu4CNHj+QjR4/MQpAiIvuHksW+eOWOoJX2\niKPgM48F/T914+5c/6eFFBXk8Y3zp2YhSBGR/UfJYm8texIe+TIcdhZ8cnav/T79ecF65q3YwlfP\nnqJGdyJywFMX5XsjEYfH/x2qJ8Klv4P8wh5Xa2qL8R+PLOLoMYN1m6yIHBSULPbGgruhYQl8/O5e\nEwXAzY8uYVtbjN9+5ggiebpNVkQOfCqGSldHKzz9HRh7Ekw5v9fVXly5hfteXsOnTx7P4SMr+zFA\nEZHM0ZVFuubdBts3weX3Bf0/9WBTc5Qv3Pc6E2vL+OKZh/VzgCIimaNkkY7m9fD8bXD4JTC657Yr\nsUSSz9/7Gq3ROHd/9gTKivTRisjBQ0e0dMz9DngCPvTNXlf5/v8t4ZVV27j1sulMHl7Rj8GJiGSe\n6iz6snUlvH4PzJwFVeN7XOWRNzdwx3PvcvVJ47hw+m7DboiIHPCULPqyaDbgcMI/9rj43c3b+epD\nb3DM2MH8+3ndR40VETk4KFn0ZckjMGJ6j2NUJJPO1x56k0iecfsVx2ogIxE5aOnotict9VD3cq+3\nyt790mpeXrWV68+fxsjBJf0cnIhI/1Gy2JOlc4K/U3dPFmu3tnHz/y3htENr+bvjdu9EUETkYKJk\nsSdLHgm69hgyZZfZ7s6//fEtDPjeJUdqMCMROegpWfQm2gwrn4Up5+3WCO/BV+v427LNXHfOFEZX\nlWYpQBGR/qNk0Ztlj0Mytlt9xebWDr79l0XMnFDNlSeMy1JwIiL9S8miN0segbKhMPr4XWbf+9Ia\nWqJxvnvxEeSpk0ARGSCULHoS74BlT8DkcyAvsnN2Ism9L63htENrOWSoWmmLyMChZNGTd/8KnS27\nFUE9uXgT9c1RPnGiip9EZGBRsujJkr9AYTlMeN8us+9+cTUjK4v54JShWQpMRCQ7lCy6SyZhyRw4\n9Ewo2Dkc6sqGVp5bvpkrThhLfkQfm4gMLDrqdbd1RTBuxSEf2mX2PS+toSBiXHr87t1+iIgc7JQs\numuqC/6m9DDb3pngwflrOaty0RIAABUDSURBVOvw4QytKO55OxGRg5iSRXfN64O/g3Z2Nf6/b6yn\nORpXxbaIDFgZTRZmdraZLTWz5WZ2XQ/Lx5rZXDN73czeNLNzw/njzazdzBaEj59nMs5dNK8L/laM\n2DHrdy+u5rBh5cycUN1vYYiI5JKMjZRnZhHgduBMoA54xcxmu/uilNWuBx5w95+Z2TRgDjA+XLbC\n3adnKr5eNa+D0todldvLN7Xw1romvnXB4eoDSkQGrExeWcwElrv7SnfvBO4HLuy2jgODwueVwPoM\nxpOe5vUwaOSOybfXNwNw4sSabEUkIpJ1mUwWo4C1KdN14bxUNwJXmVkdwVXFF1KWTQiLp541s9N6\negEzm2Vm881sfkNDw/6Junn9LvUVS+pbKIgYE4eU7Z/9i4gcgLJdwX058Bt3Hw2cC/zOzPKADcBY\ndz8G+BJwr5kN6r6xu//S3We4+4whQ4bsn4ia1+1yZbFkQzOThpRToLYVIjKAZfIIuA5IbZQwOpyX\n6rPAAwDu/gJQDNS6e4e7bwnnvwqsAA7LYKyBzjZo3waVO68slta3MHm4+oESkYEtrWRhZn8ws/PC\ns/50vQIcamYTzKwQuAyY3W2dNcAZ4WtMJUgWDWY2JKwgx8wmAocCK/fitfdNt9tmm9pjrG+KKlmI\nyICX7sH/p8AVwDIzu9nMJve1gbvHgWuAx4DFBHc9vW1mN5nZBeFqXwb+3szeAO4DPuXuDrwPeNPM\nFgAPAf/o7lv36p3ti67bZsNiqHc2tgAwdfhuJWAiIgNKWrfOuvuTwJNmVklQz/Ckma0FfgXc7e6x\nXrabQ1BxnTrvhpTni4BTetjuYeDhdN/EftPtymJJfZAsdGUhIgNd2sVKZlYDfAr4HPA6cCtwLPBE\nRiLLhm4N8pbWN1NRnM+ISnXxISIDW1pXFmb2R2Ay8DvgI+6+IVz0ezObn6ng+l3zeiiphsJgXO2l\n9S1MGV6hxngiMuCl24L7Nnef29MCd5+xH+PJrpQ2Fu7OkvoWLpw+so+NREQOfukWQ00zs8FdE2ZW\nZWb/lKGYsqe5bkfl9oamKC3ROJNVuS0iknay+Ht3b+yacPdtwN9nJqQsSunqY2lYuT1FldsiImkn\ni4ilFNyHbSAKMxNSlsSi0LZltzuhDhumZCEikm6dxaMEldm/CKf/IZx38Gjpum02uLJYUt/MyMpi\nKksKshiUiEhuSDdZfI0gQfy/cPoJ4I6MRJQtXW0swq4+1M2HiMhO6TbKSwI/Cx8Hp6au1tujiCWS\nrGho5QNThmY3JhGRHJFuO4tDge8B0wj6bwLA3SdmKK7+l9Igb2XDdmIJV+W2iEgo3QruXxNcVcSB\nDwB3AXdnKqisaF4PxZVQVM6S+mDAIxVDiYgE0k0WJe7+FGDuvtrdbwTOy1xYWZDSIG9pfQv5ecbE\n2vIsByUikhvSreDuCLsnX2Zm1xCMS3FwHUlTBj1aWt/CpCHlFOZrwCMREUj/yuJaoBT4Z+A44Crg\n6kwFlRUpVxZLdCeUiMgu+kwWYQO8j7t7q7vXufun3f2j7v5iP8TXP+IdsH0TDBpFSzTGusZ2JQsR\nkRR9Jgt3TwCn9kMs2dMSdqI7aCSbWjoAGF1VksWARERyS7p1Fq+b2WzgQWB710x3/0NGoupvzTtb\nb7d3JgAoKYhkMSARkdySbrIoBrYAH0yZ58BBlixGEW0Lk0WhkoWISJd0W3B/OtOBZFXK2NvtTUEx\nlK4sRER2SrcF968JriR24e6f2e8RZUPzeigaBMWDaO+sB6BYyUJEZId0i6H+kvK8GLgYWL//w8mS\npp2DHrXHVAwlItJdusVQD6dOm9l9wHMZiSgbUgY9isZUwS0i0t2+NlE+FDh4umRNSRa6G0pEZHfp\n1lm0sGudRT3BGBcHvkQMWjfuaL3dHksCKoYSEUmVbjHUwduceXsDRAp2q7MoUr9QIiI7pHtlcTHw\ntLs3hdODgdPd/U+ZDK5fDBoJ12+CZBwI6ixKCiKkDDkuIjLgpXv6/M2uRAHg7o3AN/vayMzONrOl\nZrbczK7rYflYM5trZq+b2Ztmdm7Ksq+H2y01s7PSjHPfmAVXFwR1FiqCEhHZVbq3zvaUVPa4bdgB\n4e3AmUAd8IqZzXb3RSmrXQ884O4/M7NpwBxgfPj8MuBwYCTwpJkdFvZTlVHt4ZWFiIjslO6VxXwz\n+5GZTQofPwJe7WObmcByd1/p7p3A/cCF3dZxYFD4vJKdbTcuBO539w53fxdYHu4v49pjCYoLVF8h\nIpIq3aPiF4BO4PcEB/0o8Pk+thkFrE2ZrgvnpboRuMrM6giuKr6wF9tiZrPMbL6ZzW9oaEjvnfQh\nqmIoEZHdpHs31HZgtzqH/eBy4Dfu/kMzOwn4nZkdke7G7v5L4JcAM2bM2K07kn2hYigRkd2ldWVh\nZk+Ed0B1TVeZ2WN9bLYOGJMyPTqcl+qzwAMA7v4CQVcitWlumxFBMZSShYhIqnSLoWrDO6AAcPdt\n9N2C+xXgUDObYGaFBBXWs7utswY4A8DMphIki4ZwvcvMrMjMJhC0GH85zVjfk/ZOXVmIiHSX7t1Q\nSTMb6+5rAMxsPD30QpvK3eNmdg3wGBAB7nT3t83sJmC+u88Gvgz8ysy+GO7vU+7uwNtm9gCwCIgD\nn++PO6EgbGehOgsRkV2kmyz+HXjOzJ4FDDgNmNXXRu4+h6DiOnXeDSnPFwGn9LLtd4DvpBnffqM6\nCxGR3aVbwf2omc0gSBCvA38C2jMZWLa0d6rOQkSku3S7+/gccC1BRfMC4ETgBXYdZvWgEI0lVQwl\nItJNuhXc1wLHA6vd/QPAMUDjnjc58MQTSToTSRVDiYh0k26yiLp7FMDMitx9CTA5c2FlRzQedk+u\nZCEisot0K7jrwnYWfwKeMLNtwOrMhZUdXQMfFasYSkRkF+lWcF8cPr3RzOYS9OP0aMaiyhINqSoi\n0rN0ryx2cPdnMxFILmhXshAR6ZG6V02xY/ztQn0sIiKpdFRM0XVloXYWIiK7UrJIoWIoEZGeKVmk\niO4ohlKyEBFJpWSRQlcWIiI9U7JIoWQhItIzJYsU7SqGEhHpkZJFiqjuhhIR6ZGSRYq2zgQFEaMg\noo9FRCSVjoopNP62iEjPlCxSRDVKnohIj5QsUrR3avxtEZGeKFmk0PjbIiI9U7JI0R5Lqs5CRKQH\nShYpop26shAR6YmSRYr2mOosRER6omSRQnUWIiI9U7JI0d6pdhYiIj1RskgRjSU0Sp6ISA8yemQ0\ns7PNbKmZLTez63pY/mMzWxA+3jGzxpRliZRlszMZZxcVQ4mI9Cw/Uzs2swhwO3AmUAe8Ymaz3X1R\n1zru/sWU9b8AHJOyi3Z3n56p+LpzdyULEZFeZPLKYiaw3N1XunsncD9w4R7Wvxy4L4Px7FFHPIk7\nFOtuKBGR3WQyWYwC1qZM14XzdmNm44AJwNMps4vNbL6ZvWhmF/Wy3axwnfkNDQ3vKdioBj4SEelV\nrtTmXgY85O6JlHnj3H0GcAVwi5lN6r6Ru//S3We4+4whQ4a8pwA0Sp6ISO8ymSzWAWNSpkeH83py\nGd2KoNx9Xfh3JfAMu9Zn7HcaJU9EpHeZTBavAIea2QQzKyRICLvd1WRmU4Aq4IWUeVVmVhQ+rwVO\nARZ133Z/atcoeSIivcrY3VDuHjeza4DHgAhwp7u/bWY3AfPdvStxXAbc7+6esvlU4BdmliRIaDen\n3kWVCaqzEBHpXcaSBYC7zwHmdJt3Q7fpG3vYbh5wZCZj6669MwmoGEpEpCe5UsGddargFhHpnZJF\nSHUWIiK9U7IIRXU3lIhIr5QsQiqGEhHpnZJFSMlCRKR3ShahrkZ5Rfn6SEREutORMRSNJSguyCMv\nz7IdiohIzlGyCKl7chGR3ilZhNo7lSxERHqjZBFqjyU0loWISC+ULEJRFUOJiPRKySKkOgsRkd4p\nWYTaOxNqvS0i0gsli1B7LKkrCxGRXihZhKIxXVmIiPRGySKkW2dFRHqnZBFqjyXUPbmISC+ULELt\nKoYSEemVkgWQSDqdcVVwi4j0RsmCoHIb1D25iEhvlCxIGVJVxVAiIj1SsmDnWBa6shAR6ZmSBRol\nT0SkL0oWpFxZFOrjEBHpiY6OpNRZ6MpCRKRHGU0WZna2mS01s+Vmdl0Py39sZgvCxztm1piy7Goz\nWxY+rs5knCqGEhHZs/xM7djMIsDtwJlAHfCKmc1290Vd67j7F1PW/wJwTPi8GvgmMANw4NVw222Z\niDW6oxhKyUJEpCeZvLKYCSx395Xu3gncD1y4h/UvB+4Ln58FPOHuW8ME8QRwdqYC1ZWFiMieZTJZ\njALWpkzXhfN2Y2bjgAnA03u77f6gZCEisme5UsF9GfCQuyf2ZiMzm2Vm881sfkNDwz6/eNfdUGqU\nJyLSs0wmi3XAmJTp0eG8nlzGziKotLd191+6+wx3nzFkyJB9DlTdfYiI7Fkmk8UrwKFmNsHMCgkS\nwuzuK5nZFKAKeCFl9mPAh82sysyqgA+H8zKiPZYgP88oiOTKhZaISG7J2N1Q7h43s2sIDvIR4E53\nf9vMbgLmu3tX4rgMuN/dPWXbrWb2bYKEA3CTu2/NVKztnepxVkRkTzKWLADcfQ4wp9u8G7pN39jL\ntncCd2YsuBTtsYTqK0RE9kDlLoTjb+vKQkSkV0oWaPxtEZG+KFmgYigRkb4oWRCOv12gj0JEpDc6\nQqI6CxGRvihZENZZqBhKRKRXShaEdRa6shAR6ZWSBSqGEhHpi5IFunVWRKQvAz5ZuHtwN5TqLERE\nejXgk0VnIknSNf62iMieDPhkEe1MAuqeXERkTwZ8ssDgvKNGMGloebYjERHJWRntdfZAUFlSwO1X\nHJvtMEREcpquLEREpE9KFiIi0iclCxER6ZOShYiI9EnJQkRE+qRkISIifVKyEBGRPilZiIhIn8zd\nsx3DfmFmDcDq97CLWmDzfgpnf8vl2CC348vl2CC348vl2CC348vl2GDX+Ma5+5C+NjhoksV7ZWbz\n3X1GtuPoSS7HBrkdXy7HBrkdXy7HBrkdXy7HBvsWn4qhRESkT0oWIiLSJyWLnX6Z7QD2IJdjg9yO\nL5djg9yOL5djg9yOL5djg32IT3UWIiLSJ11ZiIhIn5QsRESkTwM+WZjZ2Wa21MyWm9l1ORDPnWa2\nycwWpsyrNrMnzGxZ+LcqS7GNMbO5ZrbIzN42s2tzLL5iM3vZzN4I4/tWOH+Cmb0Ufse/N7PCbMQX\nxhIxs9fN7C85GNsqM3vLzBaY2fxwXq58t4PN7CEzW2Jmi83spByKbXL4mXU9ms3sX3Iovi+G/w8L\nzey+8P9kr393AzpZmFkEuB04B5gGXG5m07IbFb8Bzu427zrgKXc/FHgqnM6GOPBld58GnAh8Pvy8\nciW+DuCD7n40MB0428xOBL4P/NjdDwG2AZ/NUnwA1wKLU6ZzKTaAD7j79JR78HPlu70VeNTdpwBH\nE3yGORGbuy8NP7PpwHFAG/DHXIjPzEYB/wzMcPcjgAhwGfvyu3P3AfsATgIeS5n+OvD1HIhrPLAw\nZXopMCJ8PgJYmu0Yw1j+DJyZi/EBpcBrwAkELVXze/rO+zmm0QQHjQ8CfwEsV2ILX38VUNttXta/\nW6ASeJfwhpxciq2HWD8MPJ8r8QGjgLVANcEw2n8BztqX392AvrJg5wfZpS6cl2uGufuG8Hk9MCyb\nwQCY2XjgGOAlcii+sJhnAbAJeAJYATS6ezxcJZvf8S3AV4FkOF1D7sQG4MDjZvaqmc0K5+XCdzsB\naAB+HRbh3WFmZTkSW3eXAfeFz7Men7uvA34ArAE2AE3Aq+zD726gJ4sDjgenAlm939nMyoGHgX9x\n9+bUZdmOz90THhQHjAZmAlOyFUsqMzsf2OTur2Y7lj041d2PJSiW/byZvS91YRa/23zgWOBn7n4M\nsJ1uRTrZ/t0BhOX+FwAPdl+WrfjCepILCRLuSKCM3Yu50zLQk8U6YEzK9OhwXq7ZaGYjAMK/m7IV\niJkVECSKe9z9D7kWXxd3bwTmElxiDzaz/HBRtr7jU4ALzGwVcD9BUdStORIbsOMsFHffRFDmPpPc\n+G7rgDp3fymcfoggeeRCbKnOAV5z943hdC7E9yHgXXdvcPcY8AeC3+Je/+4GerJ4BTg0vDOgkOAS\ncnaWY+rJbODq8PnVBHUF/c7MDPgfYLG7/yhlUa7EN8TMBofPSwjqUxYTJI2PZTM+d/+6u4929/EE\nv7On3f3KXIgNwMzKzKyi6zlB2ftCcuC7dfd6YK2ZTQ5nnQEsyoXYurmcnUVQkBvxrQFONLPS8P+3\n67Pb+99dtiuEsv0AzgXeISjb/vcciOc+grLFGMEZ1WcJyrafApYBTwLVWYrtVIJL6TeBBeHj3ByK\n7yjg9TC+hcAN4fyJwMvAcoIigqIsf8enA3/JpdjCON4IH293/S/k0Hc7HZgffrd/AqpyJbYwvjJg\nC1CZMi8n4gO+BSwJ/yd+BxTty+9O3X2IiEifBnoxlIiIpEHJQkRE+qRkISIifVKyEBGRPilZiIhI\nn5QsRLLIzE7v6oFWJJcpWYiISJ+ULETSYGZXhWNlLDCzX4QdFraa2Y/DsQKeMrMh4brTzexFM3vT\nzP7YNY6BmR1iZk+G4228ZmaTwt2Xp4zVcE/Y0hYzu9mCsUPeNLMfZOmtiwBKFiJ9MrOpwMeBUzzo\npDABXEnQane+ux8OPAt8M9zkLuBr7n4U8FbK/HuA2z0Yb+Nkgpb6EPTe+y8EY6pMBE4xsxrgYuDw\ncD//kdl3KbJnShYifTuDYFCbV8Luz88gOKgngd+H69wNnGpmlcBgd382nP9b4H1hv0uj3P2PAO4e\ndfe2cJ2X3b3O3ZMEXaiMJ+hKOgr8j5ldQjCgjkjWKFmI9M2A33o4Gpq7T3b3G3tYb1/7zulIeZ4g\nGJQmTtDr60PA+cCj+7hvkf1CyUKkb08BHzOzobBjXOpxBP8/XT13XgE85+5NwDYzOy2c/wngWXdv\nAerM7KJwH0VmVtrbC4ZjhlS6+xzgiwRDiYpkTX7fq4gMbO6+yMyuJxhFLo+gR+DPEwzCMzNctomg\nXgOCLp9/HiaDlcCnw/mfAH5hZjeF+/i7PbxsBfBnMysmuLL50n5+WyJ7Rb3OiuwjM2t19/JsxyHS\nH1QMJSIifdKVhYiI9ElXFiIi0iclCxER6ZOShYiI9EnJQkRE+qRkISIiffr/2SSC2Y/FlnoAAAAA\nSUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgQREQEUCCIiElAgiIgIAP8fj2tjNu5ZLIkAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"gmSAv3tVZa9b","colab_type":"code","outputId":"ae71333e-0876-4e98-8e5d-8fb4d167e43a","executionInfo":{"status":"ok","timestamp":1578723228480,"user_tz":-60,"elapsed":2830107,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":351}},"source":["# -----------------------------\n","# Evaluation metrics on Training\n","# -----------------------------\n","\n","# obtain a list of test scans\n","test_scans = os.listdir(options['training_path'])\n","th = 0.5\n","\n","# iterate through the scans and evaluate the results\n","metrics = np.zeros((len(test_scans), 3))\n","scan_id_list = []\n","\n","for i, scan_name in enumerate(test_scans):\n","\n","    scan_path = os.path.join(options['training_path'], scan_name)\n","    scan = ants.image_read(os.path.join(scan_path, '{}.nii.gz'.format(scan_name)))\n","\n","    infer_patches, coordenates = get_inference_patches(scan_path=scan_path,\n","                                                       input_data=['{}.nii.gz'.format(scan_name)],\n","                                                       roi='{}_brainmask.nii.gz'.format(scan_name),\n","                                                       patch_shape=options['patch_size'],\n","                                                       step=options['sampling_step'],\n","                                                       normalize=options['normalize'])\n","\n","    # lesion_out = np.zeros_like(infer_patches).astype('float32')\n","    lesion_out = np.zeros((infer_patches.shape[0], 4, infer_patches.shape[2], infer_patches.shape[3],\n","                           infer_patches.shape[4]), dtype='float32')\n","    batch_size = options['batch_size']\n","\n","    # model evaluation\n","    lesion_model.eval()\n","    with torch.no_grad():\n","        for b in range(0, len(lesion_out), batch_size):\n","            x = torch.tensor(infer_patches[b:b + batch_size]).to(device)\n","            pred = lesion_model(x)\n","            lesion_out[b:b + batch_size] = pred.cpu().numpy()\n","\n","    # reconstruct image takes the inferred patches, the patches coordenates and the image size as inputs\n","    lesion_prob = np.expand_dims(reconstruct_image(lesion_out[:, 0],\n","                                                   coordenates,\n","                                                   scan.shape), axis=0)\n","    lesion_prob2 = np.expand_dims(reconstruct_image(lesion_out[:, 1],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","    lesion_prob3 = np.expand_dims(reconstruct_image(lesion_out[:, 2],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","    lesion_prob4 = np.expand_dims(reconstruct_image(lesion_out[:, 3],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","\n","    tissue_seg = np.stack((lesion_prob, lesion_prob2, lesion_prob3, lesion_prob4), axis=0).squeeze()\n","\n","    tissue_seg = np.argmax(tissue_seg, axis=0)\n","\n","    CSF = tissue_seg == 1\n","    GM = tissue_seg == 2\n","    WM = tissue_seg == 3\n","\n","    # binarize the results\n","    lesion_prob = (lesion_prob > th).astype('uint8')\n","\n","    # evaluate the results\n","    gt = ants.image_read(os.path.join(scan_path, '{}_seg.nii.gz'.format(scan_name)))\n","    dsc_CSF = DSC_seg(gt.numpy() == 1, CSF)\n","    dsc_GM = DSC_seg(gt.numpy() == 2, GM)\n","    dsc_WM = DSC_seg(gt.numpy() == 3, WM)\n","    # tpf_metric = TPF_det(gt.numpy() == 1, lesion_prob > 0)\n","    # ppv_metric = PPV_det(gt.numpy() == 1, lesion_prob > 0)\n","\n","    metrics[i] = [dsc_CSF, dsc_GM, dsc_WM]\n","    scan_id_list.append(scan_name)\n","\n","    print('SCAN:', scan_name, 'dice_CSF: ', dsc_CSF, 'dice_GM:', dsc_GM, 'dice_WM:', dsc_WM)\n","\n","    # # save as nifti image is necessary\n","    seg_img = ants.from_numpy(tissue_seg.astype('uint8'))\n","    seg_img = ants.copy_image_info(scan, seg_img)\n","    # write segmented image to folder\n","    # seg_path = \"{}/{}.nii.gz\".format(tmpdir, scan_name)\n","    # ants.image_write(seg_img, seg_path)\n","\n","\n","# we use PANDAS to describe data :)\n","metrics_df = {'scan_id': scan_id_list, 'DSC_CSF': metrics[:, 0], 'DSC_GM': metrics[:, 1], 'DSC_WM': metrics[:, 2]}\n","m = pd.DataFrame(metrics_df, columns=['scan_id', 'DSC_CSF', 'DSC_GM', 'DSC_WM'])\n","m_mean = m.describe().T\n","\n","\n","diceAll_csv_path = \"{}/dice_all_train.csv\".format(tmpdir)\n","diceMean_csv_path = \"{}/dice_mean_train.csv\".format(tmpdir)\n","m.to_csv(diceAll_csv_path)\n","m_mean.to_csv(diceMean_csv_path)\n","\n","# handlers = logger.handlers[:]\n","# for handler in handlers:\n","#     handler.close()\n","#     logger.removeHandler(handler)\n","m_mean"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MISA_FinalProject_Abdullah_Pierpaolo_Prem/utils.py:462: RuntimeWarning: invalid value encountered in true_divide\n","  out_image /= freq_count\n"],"name":"stderr"},{"output_type":"stream","text":["SCAN: IBSR_16 dice_CSF:  0.9273466649897287 dice_GM: 0.9751084937826228 dice_WM: 0.9718927441789565\n","SCAN: IBSR_08 dice_CSF:  0.9591064340441244 dice_GM: 0.9606802256921664 dice_WM: 0.9721165988860555\n","SCAN: IBSR_06 dice_CSF:  0.9455516653200466 dice_GM: 0.9560074171894481 dice_WM: 0.9626772756711033\n","SCAN: IBSR_18 dice_CSF:  0.9421988768603335 dice_GM: 0.9739324954091815 dice_WM: 0.9732470646682484\n","SCAN: IBSR_04 dice_CSF:  0.9121592909609665 dice_GM: 0.9668276963345883 dice_WM: 0.9616821658461524\n","SCAN: IBSR_03 dice_CSF:  0.9002175489485135 dice_GM: 0.9694142812578915 dice_WM: 0.9625484953477865\n","SCAN: IBSR_05 dice_CSF:  0.9377578078962876 dice_GM: 0.957521005530351 dice_WM: 0.9606489763982938\n","SCAN: IBSR_07 dice_CSF:  0.9400009607532306 dice_GM: 0.963715937516909 dice_WM: 0.9734246714832158\n","SCAN: IBSR_01 dice_CSF:  0.9485167850769386 dice_GM: 0.970478925526254 dice_WM: 0.9699822525906405\n","SCAN: IBSR_09 dice_CSF:  0.9570654138055656 dice_GM: 0.9599709036435264 dice_WM: 0.9661672929902316\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>DSC_CSF</th>\n","      <td>10.0</td>\n","      <td>0.936992</td>\n","      <td>0.018847</td>\n","      <td>0.900218</td>\n","      <td>0.929949</td>\n","      <td>0.941100</td>\n","      <td>0.947776</td>\n","      <td>0.959106</td>\n","    </tr>\n","    <tr>\n","      <th>DSC_GM</th>\n","      <td>10.0</td>\n","      <td>0.965366</td>\n","      <td>0.006798</td>\n","      <td>0.956007</td>\n","      <td>0.960148</td>\n","      <td>0.965272</td>\n","      <td>0.970213</td>\n","      <td>0.975108</td>\n","    </tr>\n","    <tr>\n","      <th>DSC_WM</th>\n","      <td>10.0</td>\n","      <td>0.967439</td>\n","      <td>0.005220</td>\n","      <td>0.960649</td>\n","      <td>0.962581</td>\n","      <td>0.968075</td>\n","      <td>0.972061</td>\n","      <td>0.973425</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         count      mean       std  ...       50%       75%       max\n","DSC_CSF   10.0  0.936992  0.018847  ...  0.941100  0.947776  0.959106\n","DSC_GM    10.0  0.965366  0.006798  ...  0.965272  0.970213  0.975108\n","DSC_WM    10.0  0.967439  0.005220  ...  0.968075  0.972061  0.973425\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"2d_IAtB0c-qX","colab_type":"text"},"source":["# 5. Evaluation:\n","---\n","\n","In order to evaluate the results, we use standard metric for lesion segmentation such as:\n","* `DSC:` Dice **segmentation** coefficient \n","\n","Let's infer all the test images and evaluate the results in average calculating the Dice Score for: \n","* `CSF`\n","* `GM`\n","* `WM`"]},{"cell_type":"code","metadata":{"id":"BsO-nSJGc-qY","colab_type":"code","outputId":"2c778db3-02b1-403f-9b0f-e8e3dd5d6302","executionInfo":{"status":"ok","timestamp":1578723248278,"user_tz":-60,"elapsed":2848447,"user":{"displayName":"Abdullah Thabit","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mClxJP6R_nRLtZOmVgnver4laTzdsMFk0BptEZS=s64","userId":"07023115758106604748"}},"colab":{"base_uri":"https://localhost:8080/","height":262}},"source":["# -----------------------------\n","# Evaluation metrics\n","# -----------------------------\n","\n","# obtain a list of test scans\n","test_scans = os.listdir(options['validation_path'])\n","th = 0.5\n","\n","# iterate through the scans and evaluate the results\n","metrics = np.zeros((len(test_scans), 3))\n","scan_id_list = []\n","\n","for i, scan_name in enumerate(test_scans):\n","\n","    scan_path = os.path.join(options['validation_path'], scan_name)\n","    scan = ants.image_read(os.path.join(scan_path, '{}.nii.gz'.format(scan_name)))\n","\n","    infer_patches, coordenates = get_inference_patches(scan_path=scan_path,\n","                                                       input_data=['{}.nii.gz'.format(scan_name)],\n","                                                       roi='{}_brainmask.nii.gz'.format(scan_name),\n","                                                       patch_shape=options['patch_size'],\n","                                                       step=options['sampling_step'],\n","                                                       normalize=options['normalize'])\n","\n","    # lesion_out = np.zeros_like(infer_patches).astype('float32')\n","    lesion_out = np.zeros((infer_patches.shape[0], 4, infer_patches.shape[2], infer_patches.shape[3],\n","                           infer_patches.shape[4]), dtype='float32')\n","    batch_size = options['batch_size']\n","\n","    # model evaluation\n","    lesion_model.eval()\n","    with torch.no_grad():\n","        for b in range(0, len(lesion_out), batch_size):\n","            x = torch.tensor(infer_patches[b:b + batch_size]).to(device)\n","            pred = lesion_model(x)\n","            lesion_out[b:b + batch_size] = pred.cpu().numpy()\n","\n","    # reconstruct image takes the inferred patches, the patches coordenates and the image size as inputs\n","    lesion_prob = np.expand_dims(reconstruct_image(lesion_out[:, 0],\n","                                                   coordenates,\n","                                                   scan.shape), axis=0)\n","    lesion_prob2 = np.expand_dims(reconstruct_image(lesion_out[:, 1],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","    lesion_prob3 = np.expand_dims(reconstruct_image(lesion_out[:, 2],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","    lesion_prob4 = np.expand_dims(reconstruct_image(lesion_out[:, 3],\n","                                                    coordenates,\n","                                                    scan.shape), axis=0)\n","\n","    tissue_seg = np.stack((lesion_prob, lesion_prob2, lesion_prob3, lesion_prob4), axis=0).squeeze()\n","\n","    tissue_seg = np.argmax(tissue_seg, axis=0)\n","\n","    CSF = tissue_seg == 1\n","    GM = tissue_seg == 2\n","    WM = tissue_seg == 3\n","\n","    # binarize the results\n","    lesion_prob = (lesion_prob > th).astype('uint8')\n","\n","    # evaluate the results\n","    gt = ants.image_read(os.path.join(scan_path, '{}_seg.nii.gz'.format(scan_name)))\n","    dsc_CSF = DSC_seg(gt.numpy() == 1, CSF)\n","    dsc_GM = DSC_seg(gt.numpy() == 2, GM)\n","    dsc_WM = DSC_seg(gt.numpy() == 3, WM)\n","    # tpf_metric = TPF_det(gt.numpy() == 1, lesion_prob > 0)\n","    # ppv_metric = PPV_det(gt.numpy() == 1, lesion_prob > 0)\n","\n","    metrics[i] = [dsc_CSF, dsc_GM, dsc_WM]\n","    scan_id_list.append(scan_name)\n","\n","    print('SCAN:', scan_name, 'dice_CSF: ', dsc_CSF, 'dice_GM:', dsc_GM, 'dice_WM:', dsc_WM)\n","\n","    # # save as nifti image is necessary\n","    seg_img = ants.from_numpy(tissue_seg.astype('uint8'))\n","    seg_img = ants.copy_image_info(scan, seg_img)\n","    # write segmented image to folder\n","    seg_path = \"{}/{}.nii.gz\".format(tmpdir, scan_name)\n","    ants.image_write(seg_img, seg_path)\n","\n","\n","# we use PANDAS to describe data :)\n","metrics_df = {'scan_id': scan_id_list, 'DSC_CSF': metrics[:, 0], 'DSC_GM': metrics[:, 1], 'DSC_WM': metrics[:, 2]}\n","m = pd.DataFrame(metrics_df, columns=['scan_id', 'DSC_CSF', 'DSC_GM', 'DSC_WM'])\n","m_mean = m.describe().T\n","\n","\n","diceAll_csv_path = \"{}/dice_all_val.csv\".format(tmpdir)\n","diceMean_csv_path = \"{}/dice_mean_val.csv\".format(tmpdir)\n","m.to_csv(diceAll_csv_path)\n","m_mean.to_csv(diceMean_csv_path)\n","m_mean\n","# handlers = logger.handlers[:]\n","# for handler in handlers:\n","#     handler.close()\n","#     logger.removeHandler(handler)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/MISA_FinalProject_Abdullah_Pierpaolo_Prem/utils.py:462: RuntimeWarning: invalid value encountered in true_divide\n","  out_image /= freq_count\n"],"name":"stderr"},{"output_type":"stream","text":["SCAN: IBSR_11 dice_CSF:  0.9065187239944521 dice_GM: 0.9499878615991091 dice_WM: 0.9657157708022376\n","SCAN: IBSR_17 dice_CSF:  0.9463829447460875 dice_GM: 0.9542541697533219 dice_WM: 0.9407841476765385\n","SCAN: IBSR_14 dice_CSF:  0.9304174950298211 dice_GM: 0.9588580390995936 dice_WM: 0.9542128141452816\n","SCAN: IBSR_13 dice_CSF:  0.8986047357178567 dice_GM: 0.9476670918751977 dice_WM: 0.9333250145578571\n","SCAN: IBSR_12 dice_CSF:  0.9225724922299259 dice_GM: 0.9403830365381401 dice_WM: 0.9495068151852064\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","      <th>mean</th>\n","      <th>std</th>\n","      <th>min</th>\n","      <th>25%</th>\n","      <th>50%</th>\n","      <th>75%</th>\n","      <th>max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>DSC_CSF</th>\n","      <td>5.0</td>\n","      <td>0.920899</td>\n","      <td>0.019017</td>\n","      <td>0.898605</td>\n","      <td>0.906519</td>\n","      <td>0.922572</td>\n","      <td>0.930417</td>\n","      <td>0.946383</td>\n","    </tr>\n","    <tr>\n","      <th>DSC_GM</th>\n","      <td>5.0</td>\n","      <td>0.950230</td>\n","      <td>0.006968</td>\n","      <td>0.940383</td>\n","      <td>0.947667</td>\n","      <td>0.949988</td>\n","      <td>0.954254</td>\n","      <td>0.958858</td>\n","    </tr>\n","    <tr>\n","      <th>DSC_WM</th>\n","      <td>5.0</td>\n","      <td>0.948709</td>\n","      <td>0.012446</td>\n","      <td>0.933325</td>\n","      <td>0.940784</td>\n","      <td>0.949507</td>\n","      <td>0.954213</td>\n","      <td>0.965716</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         count      mean       std  ...       50%       75%       max\n","DSC_CSF    5.0  0.920899  0.019017  ...  0.922572  0.930417  0.946383\n","DSC_GM     5.0  0.950230  0.006968  ...  0.949988  0.954254  0.958858\n","DSC_WM     5.0  0.948709  0.012446  ...  0.949507  0.954213  0.965716\n","\n","[3 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":18}]}]}